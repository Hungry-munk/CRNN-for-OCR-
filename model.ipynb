{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Conv2D, MaxPooling2D, Dropout, Lambda\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from configs import Configs \n",
    "from data_processing import data_preparator\n",
    "%run \"tester_functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Configs()\n",
    "batch_size = c.batch_size\n",
    "batch_size = 50\n",
    "image_size = c.form_height\n",
    "image_size = 128\n",
    "\n",
    "X, Y = data_preparator(c.image_paths, c.label_path, image_target_height = image_size, batch_size = batch_size, augmentation_probability = c.augmentation_probability )\n",
    "print(len(X))\n",
    "print(Y.shape)\n",
    "inpute_batch_displayer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Configs()\n",
    "# retrive precessed data that can be used for training \n",
    "X, Y = data_preparator(c.image_paths, c.label_path, image_target_height = c.image_height, image_target_width = c.image_width, batch_size = c.batch_size, augmentation_probability = c.augmentation_probability )\n",
    "\n",
    "# scramble arraies but keep corresponding indecies\n",
    "# Generate a random permutation of indices\n",
    "shuffled_indices = np.random.permutation(c.batch_size)\n",
    "\n",
    "# X = X[shuffled_indices]\n",
    "# Y = Y[shuffled_indices]\n",
    "# split data set into training, cross validation and test sets\n",
    "# training sets split \n",
    "train_split = int(0.75 * c.batch_size)\n",
    "X_train = X[:train_split]\n",
    "Y_train = Y[:train_split]\n",
    "# Cross validation sets\n",
    "CV_test_split = int(0.125 * c.batch_size)\n",
    "X_cv = X[train_split: train_split + CV_test_split]\n",
    "Y_cv = Y[train_split: train_split + CV_test_split]\n",
    "# testing sets\n",
    "X_test = X[train_split + CV_test_split:]\n",
    "Y_test = Y[train_split + CV_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting feature maps into seqeunces\n",
    "def f_map_to_seq(f_map):\n",
    "    # Get dynamic shape (to potentially handle None for batch size)\n",
    "    batch_size ,height, width, channels  = tf.unstack(tf.shape(f_map))\n",
    "    # Flatten height and width into one dimension, keeping channels to make sequence \n",
    "    sequence = tf.reshape(f_map, (batch_size, height * width, channels))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CRNN_model(input_shape, num_classes):\n",
    "    # TODO figure out input\n",
    "    inputs = Input(shape = input_shape)\n",
    "    # CNN\n",
    "    # CNN layers inspired by the VGG architecture to create feature mappings\n",
    "    f_maps = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max1')(f_maps) # pull size for maintaining vertical information\n",
    "    # 2nd conv layer\n",
    "    f_maps = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max2')(f_maps)\n",
    "    # 3rd conv layer \n",
    "    f_maps = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max3')(f_maps)  \n",
    "    # 4th conv layer\n",
    "    f_maps = Conv2D(512, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max4')(f_maps)\n",
    "\n",
    "    # Dropout layer to help prevent overfitting\n",
    "    f_maps = Dropout(0.25)(f_maps)\n",
    "\n",
    "    # CNN to RNN transition\n",
    "    # use lambda layer and reshape function to create sequence from the feature map\n",
    "    sequence = Lambda(f_map_to_seq)(f_maps)\n",
    "\n",
    "    # RNN layers\n",
    "    # 2 sets of biderectional LSTM layers to allow for fruther and more nauiced learning \n",
    "    sequence = Bidirectional(LSTM(256, return_sequences=True))(sequence)\n",
    "    sequence = Dropout(0.25)(sequence) \n",
    "    sequence = Bidirectional(LSTM(256, return_sequences=True))(sequence)\n",
    "    # final dense layer for character probabilities using softmax output\n",
    "    Dense(num_classes, activation='softmax')(sequence)\n",
    "    # create model\n",
    "    model = Model(inputs = inputs, outputs = sequence)\n",
    "    \n",
    "    # return full model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
