{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Conv2D, MaxPooling2D, Dropout, Lambda\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from configs import Configs \n",
    "from data_processing import data_preparator, create_dataset\n",
    "%run \"tester_functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all for GPU dynamic VRAM allocation \n",
    "K.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_error_rate(y_true, y_pred):\n",
    "    # Assuming y_true and y_pred are already in index form, not one-hot encoded\n",
    "    y_true = K.get_value(y_true)\n",
    "    y_pred = K.get_value(y_pred)\n",
    "    \n",
    "    cer = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        # Filter out the blank labels (typically 0 for CTC)\n",
    "        true_str = ''.join([chr(char) for char in true if char != 0])\n",
    "        pred_str = ''.join([chr(char) for char in pred if char != 0])\n",
    "        \n",
    "        # Calculate CER using Levenshtein distance\n",
    "        edit_distance = levenshtein_distance(true_str, pred_str)\n",
    "        cer.append(edit_distance / len(true_str) if len(true_str) > 0 else 0)\n",
    "\n",
    "    return np.mean(cer)\n",
    "\n",
    "def word_error_rate(y_true, y_pred):\n",
    "    # Assuming y_true and y_pred are already in index form, not one-hot encoded\n",
    "    y_true = K.get_value(y_pred)\n",
    "    y_pred = K.get_value(y_pred)\n",
    "    \n",
    "    wer = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        # Decode the predictions and ground truths to strings\n",
    "        true_str = ''.join([chr(char) for char in true if char != 0])\n",
    "        pred_str = ''.join([chr(char) for char in pred if char != 0])\n",
    "        \n",
    "        # Split into words\n",
    "        true_words = true_str.split()\n",
    "        pred_words = pred_str.split()\n",
    "        \n",
    "        # Calculate WER using Levenshtein distance\n",
    "        edit_distance = levenshtein_distance(true_words, pred_words)\n",
    "        wer.append(edit_distance / len(true_words) if len(true_words) > 0 else 0)\n",
    "\n",
    "    return np.mean(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC loss function\n",
    "def ctc_loss_lambda_func(y_true, y_pred):\n",
    "    input_length = K.ones_like(y_pred[:, 0, 0]) * (K.int_shape(y_pred)[1])\n",
    "    label_length = K.sum(K.cast(K.not_equal(y_true, -1), 'int32'), axis=-1)\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_map_to_seq(f_map):\n",
    "    # Get dynamic shape\n",
    "    shape = tf.shape(f_map)  # Use dynamic shape to handle None dimensions\n",
    "    batch_size, height, width, channels = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    # Reshape into (batch_size, timesteps, features)\n",
    "    sequence = tf.reshape(f_map, (batch_size, width, height * channels))\n",
    "    \n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CRNN_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers\n",
    "    f_maps = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max1')(f_maps) # maintain vertical information\n",
    "    \n",
    "    f_maps = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max2')(f_maps)\n",
    "    \n",
    "    f_maps = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max3')(f_maps)\n",
    "    \n",
    "    f_maps = Conv2D(512, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(f_maps)\n",
    "    f_maps = BatchNormalization()(f_maps)\n",
    "    f_maps = Activation('relu')(f_maps)\n",
    "    f_maps = MaxPooling2D(pool_size=(1, 2), name='max4')(f_maps)\n",
    "\n",
    "    # Dropout\n",
    "    f_maps = Dropout(0.3)(f_maps)\n",
    "\n",
    "    # CNN to RNN transition: convert the feature maps into sequences\n",
    "    sequence = Lambda(f_map_to_seq)(f_maps)\n",
    "\n",
    "    # RNN layers (Bidirectional LSTMs)\n",
    "    sequence = Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='glorot_uniform'))(sequence)\n",
    "    sequence = Dropout(0.3)(sequence)\n",
    "    sequence = Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='glorot_uniform'))(sequence)\n",
    "\n",
    "    # Dense layer with softmax activation for classification\n",
    "    outputs = Dense(num_classes, activation='softmax')(sequence)\n",
    "\n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"random_contrast\" \"                 f\"(type RandomContrast).\n\n{{function_node __wrapped__Maximum_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Maximum]\n\nCall arguments received by layer \"random_contrast\" \"                 f\"(type RandomContrast):\n  • inputs=tf.Tensor(shape=(1550, 2056, 1), dtype=uint8)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m data_size \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mdata_size\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# retrive precessed data that can be used for training \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mdata_preparator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_target_height\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation_probability\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentation_probability\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.85\u001b[39m \u001b[38;5;241m*\u001b[39m c\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m      7\u001b[0m X_train_split \u001b[38;5;241m=\u001b[39m X[:train_split]\n",
      "File \u001b[1;32me:\\code\\OCR project\\data_processing.py:227\u001b[0m, in \u001b[0;36mdata_preparator\u001b[1;34m(X_image_paths, Y_image_path, data_size, image_target_height, augmentation_probability)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# augment images\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m augmentation_probability:\n\u001b[1;32m--> 227\u001b[0m     CW_form_image \u001b[38;5;241m=\u001b[39m \u001b[43maugmentation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCW_form_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m augmentation_probability:\n\u001b[0;32m    229\u001b[0m     HW_form_image \u001b[38;5;241m=\u001b[39m augmentation_model(HW_form_image)\n",
      "File \u001b[1;32mc:\\Users\\arsam\\conda\\envs\\OCR-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\arsam\\conda\\envs\\OCR-env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"random_contrast\" \"                 f\"(type RandomContrast).\n\n{{function_node __wrapped__Maximum_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Maximum]\n\nCall arguments received by layer \"random_contrast\" \"                 f\"(type RandomContrast):\n  • inputs=tf.Tensor(shape=(1550, 2056, 1), dtype=uint8)\n  • training=True"
     ]
    }
   ],
   "source": [
    "c = Configs()\n",
    "data_size = c.data_size\n",
    "# retrive precessed data that can be used for training \n",
    "X, Y = data_preparator(c.image_paths, c.label_path, image_target_height = c.image_height, data_size = data_size, augmentation_probability = c.augmentation_probability )\n",
    "\n",
    "train_split = int(0.85 * c.batch_size)\n",
    "X_train_split = X[:train_split]\n",
    "Y_train_split = Y[:train_split]\n",
    "# Cross validation sets\n",
    "CV_test_split = int(0.075 * c.batch_size)\n",
    "X_cv_split = X[train_split: train_split + CV_test_split]\n",
    "Y_cv_split = Y[train_split: train_split + CV_test_split]\n",
    "# testing sets\n",
    "X_test_split = X[train_split + CV_test_split:]\n",
    "Y_test_split = Y[train_split + CV_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for creating tensorflow datasets to allow for variable images nad ground truth labels\n",
    "batch_size = c.batch_size #how many training examples should be in one batch\n",
    "train_dataset = create_dataset(X_train_split, Y_train_split, batch_size)\n",
    "# shuffle training dataset for as more random data during training will probs help...\n",
    "buffer_size = 3000\n",
    "train_dataset = train_dataset.shuffle(buffer_size=buffer_size)\n",
    "# create crossvalidation set\n",
    "cv_dataset = create_dataset(X_cv_split, Y_cv_split, batch_size)\n",
    "# create test set\n",
    "test_dataset = create_dataset(X_test_split, Y_test_split, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model and get it ready for training\n",
    "model = build_CRNN_model((c.image_height, None, 1), c.num_classes)\n",
    "learn_rate = c.learning_rate\n",
    "# define the model optimizer, loss function and metrics we want to track\n",
    "model.compile(optimizer=Adam(learning_rate=learn_rate),\n",
    "              loss=ctc_loss_lambda_func,\n",
    "              metrics=['accuracy' , character_error_rate, word_error_rate])\n",
    "\n",
    "# Callbacks for selecting the best model and early stopping if more training does nothing \n",
    "checkpoint = ModelCheckpoint('OCR model', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs for training \n",
    "epochs = c.epoch_num \n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=cv_dataset,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to be able to import later\n",
    "model.save('OCR model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
